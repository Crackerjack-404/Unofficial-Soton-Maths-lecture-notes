\title{MATH1049: Linear Algebra II }
\author{Shub Das \\ sd2g25@soton.ac.uk}
\date{\today}

\maketitle



\tableofcontents 

\chapter{Proofs}

\section{Misconceptions}

This chapter was not something which was covered explicitly in lectures. But I still feel that it is important to learn how to write proofs, in fact, how to write and understand proofs. 

With my experience of linear algebra so far, I found that most confusion in proofs comes from forgetting what objects we're working with. 

Eg: 
\begin{itemize}
    \item Saying \textit{$U$ is a subspace} means checking three specific properties 
    \item Assuming \textit{$x \in Col(A)$} means $x = A u$ for some vector $u$ 
\end{itemize}

The point I'm trying to make is that most proofs begins by assuming the hypotheses and nothing else. So we must avoid assuming what we're trying to prove. If the goal is to show that a set is a subspace, then we cannot assume closure until we have demonstrated it. 

\chapter{Groups}

\section{Introduction}

Before I even define what a group is, \textbf{\href{https://www.southampton.ac.uk/courses/2026-27/modules/math2003}{MATH2003 Group Theory}} is a second year module which goes into a lot more detail about group theory. 

If and when I write `this will be useful later', I will try and briefly mention why. 

\subsection{Cartesian Product}

The Cartesian product is something that comes useful in both algebra and geometry. Eg: when $\mathbb{R}^2 = \mathbb{R} \times \mathbb{R}$ is both an algebraic construction (ordered pairs) and a geometric object (plane). Group theory continues this trend by abstracting operations rather than spaces. 

\subsection{Binary operations}

A \textbf{binary operation} on a set $S$ is a function 
$$
\ast : S \times S \rightarrow S 
$$

The key point here is \textbf{closure} - we combine two elements of a set which produces another element in the same set. Closure is another axiom satisfied by groups, which comes under binary operation.

One might wonder why the term binary is used here, and this is simply because the operation reflects the number of inputs- two inputs in, one out. This ensures that other axioms like associativity, identity and inverse made sense.

% how many binary operations + why? 
Given a finite set with $n$ elements, there are $n^{n^2}$ binary operations in that group. 

\subsection{Well-defined operation}

This term has come up before in Number Theory when working with equivalence classes and modular arithmetic. 

An operation is called \textbf{well-defined} if its outputs depends only on the inputs, and not how those inputs are represented. An operation that is not well defined cannot form the basis of a group, since the result of combining two elements would be ambiguous. 

\section{Definitions}

A \textbf{group} is a set $G$ with a binary operation $\ast$ that satisfies the axioms of: 

\begin{itemize}
    \item Associativity: $(a * b ) * c = a * (b * c)$
    \item Identity: $\forall a \in G, \exists e \in G \ \ s.t. \ \ a * e = G$
    \item Inverse: $\forall a \in G,  \exists b \in G \ \ s.t \ \ a * b = e $
\end{itemize}

Associativity is often taken for granted as an axiom, but it's what allows expressions involving many elements to be well-defined. Without this axiom, defining powers becomes problematic. 

\subsection{Uniqueness}

Every inverse is defined relative to the identity. In lecture notes and often in groups, only a right-inverse is assumed: $a \ast b = e$

But using associativity, we can show that this also guarantees a left inverse, and that these two inverses are unique and equal to each other. 

I won't go into detail, but the identity element and inverses in a group are unique. 

\subsection{Identity Elements}

Something that I thought it worth pointing out as mentioned in one of the problem classes, the group axioms don't tell us what the identity is, only wha it is. 

So an element $e$ is the identity if $e \ast a = a \text { and } a \ast e = a , \forall a$ 

So in a group table, the identity row and column reproduce the row and column headers, and there will be exactly one such element. 

\subsection{Abelian Groups}

A group is called \textbf{abelian} if 
$$
a * b = b * a , \ \ \forall a,b \in G 
$$

Notice how this is the same as a group being commutative. However, it's important to note that we should never assume commutativity unless it is explicitly stated. Many familiar numerical groups are abelian, many matrix and symmetry groups are not. 

The sets 
$$
Q^* = Q \setminus \set{0} , \ \ R^* = R \setminus \set{0} 
$$

form an abelian group under multiplication

Zero must be removed from these sets as it has no multiplicative inverses. 

For an abelian group, the group table will be symmetric about the diagonal. 

\subsection{Finite groups}

A group is said to be \textbf{finite} if it has finitely many elements. They often appear in combinatorics, cryptography and puzzles. 

\subsection{Multiplication tables and Sudoku}

In finite groups, we can have multiplication tables that encode the entire group structure. Each row and column must contain every element exactly once, which makes sense due to invertibility. 

\textbf{Note: }If something is a group, then it follows the Sudoku rules, but if it obeys Sudoku rules, it may not necessarily be a group. So Sudoku satisfies the necessary conditions for a group table, but not necessary ones\footnote{The axiom that often gets broken in Sudoku is the associativity one}. 

Other properties such as inverses and exponential laws are summarised well in lecture notes in \textbf{Proposition 1.4}

What I will point out is that the reversal or the order also appears in matrix inverses, function composition and transposing products. 

To think about exponent laws, think of what we're actually doing when it abstract the operation away from the numbers. I.e. we're repeating the same operation repeatedly. So exponents are really laws about repetition in an associative system with identity and inverses, which is by definition of a group! 

\section{Cyclic Groups}

Recall that an \textbf{order} of a group is just the number of elements in that group. And that the \textbf{order of an element} is simply how many times we need to combine the element with itself to get back to the identity for the first time. 

Everything is measured relative to the identity. So if we have a group table, we can spot the identity by looking at the row and column that reproduces the row and column headers. 

The most intuitive way to explain cyclic groups is to think of integers modulo $n$. Group Theory in this way actually comes from Number Theory.

More formally, a group $G$ is cyclic if: 
$$
G = \langle a \rangle = {a^n : n \in \mathbb{Z}}
$$

So every element in the group is obtained by repeatedly applying the operation to a single element. 



\subsection{Group order and elements}

Another interesting thing is that the number of elements in a group doesn't always match the order in that group. Fun example below: 
$$
C_2 \times C_2 \neq C_4
$$

The above can be verified by doing question $4$ from Problem Sheet 2. 

Another way to think about it is that the group $C_4$ is cyclic, i.e. the elements in that group are addition $ \mod 4$. And we if look at the element order, we will have elements of order $4$. 

However, the group $C_2 \times C_2$ has every non-identity element of order $2$. So the group is not cyclic. 

Both groups have element $4$, but their order is different. 

Linking back to Number Theory, we can also see that for a cyclic group $C_n$, the order of an element $k$ is the smallest positive integer $m$ s.t. 
$$
mk \equiv 0 \mod n
$$

So the group itself has order $n$, but not every element has order $n$ 

\section{Symmetries}

When we use the term symmetry of an object, we essentially talk about an action that rearranges the object, but leaves it looking the same. 

Think about rotating a square, reflecting a triangle, or shuffling positions. It is a beautiful thing to notice that symmetries of an object form a group. Why you ask?  
\begin{itemize}
    \item Doing two symmetries in a row is still a symmetry 
    \item Every symmetry can be undone 
    \item There is a `do nothing' symmetry
\end{itemize}

And notice how these are exactly the group axioms! 

\subsection{Symmetry to Permutations}

Instead of thinking about geometry, think about positions: 
$$
\set{ 1, 2, ... , n}
$$

A symmetry now becomes a matter of rearranging items, which we call \textbf{permutation}

So symmetry and permutation groups are basically the same idea, just described differently. 

More formally speaking, a permutation on a set $X$ is a \textbf{bijection} 
$$
\sigma : X \rightarrow X 
$$

Recall we defined what it meant for functions to be a bijection in Calculus I, similarly, a bijection for a group means that: 
\begin{itemize}
    \item injective: no two elements go to the same option 
    \item surjective: every option is achieved 
\end{itemize}

So in other words - nothing is lost, and nothing is duplicated. Bijection is the property that ensures an 
inverse exists and that permutations can be undone. 


\subsection{Composition}

The group operation in permutation groups is composition of functions. So if $\sigma, \tau$ are some permutatoins, then: 
$$
(\sigma \circ \tau )(x) = \sigma(\tau(x))
$$
Notice how this is associative, as function composition always it, but not commutative in general. 

It's also important to know that we apply permutations right to left. 

\subsection{Symmetric groups S_n}

The symmetric group $S_n$ is simply the group of all permutations $\set{1,2,..n}$

The order of this group is (and this comes up often in exam) is: 
$$
| S_n| = n!
$$

\subsection{Notations}

There are two main types of notation: vector and cycle. 

Both notations are explained in the notes, and I can't say any more than what's already written, except to practice doing questions involving joint and disjoint cycles. 

Cycle notation is not unique, so order matters! 

\subsection{Sign of a permutation}

The sign of a permutation tells us whether the permutation preserves the orientation or flips it. Every permutation can be built from swaps of two elements, which we define as a \textbf{transposition}. 

So the question the sign, $sgn(\sigma)$, here is trying to answer is whether we need an odd or an even number of swaps to build the permutation. 

This is defined as: 
$$
sgn(\sigma) = \begin{cases}
    +1 \text{ if $\sigma$ is even} \\
    -1 \text { if $\sigma$ is odd}
\end{cases}
$$

The even and odd here refer to the even and odd number of transpositions. 

This is somewhat like the determinant. If we think about it, swapping two rows of a matrix flips the sign, and the sign also tracks orientations. So $\det$ is the weight sum over permutations, with the sign recording the parity. This will be covered later in the course so do keep an eye out for it!

\subsubsection{Side tangent to permutation ciphers}

It absolutely blows my mind that permutation ciphers are basically permutation groups and all the things discussed above about cycles, inverse, composition show up in this. 

A permutation cipher fixes a block of position, say $n$ letters, then rearranges those position using a permutation $\sigma \in S_n$ 

So if the plaintext block is: 
$$
(x_1, x_2,... x_n)
$$

The encryption is: 
$$
(x_1, x_2,... x_n) \rightarrow (x_{\sigma^{-1}(1)},...x_{\sigma^{-1}(n)}) 
$$

And bijection ensures that these ciphers are decryptable! 


\chapter{Fields and Vector Spaces}

\section{Fields}

A field, defined in this context, is not really the green space we may or may not be able to imagine. Anyway, it's not just a list a of axioms, it's kind of a system in which things are reversible. As a fair warning, linear algebra is going to get really abstract, so I will try my best to explain how things made (or didn't) make sense to me. 

Formally, a \textbf{field} \mathbf{F} is: 
\begin{itemize}
    \item An \textbf{abelian group} under addition 
    \item a multiplicative group on F \setminus \set{0} 
    \item Multiplication distributive over addition
\end{itemize}

When looking at any axioms, it is worth thinking about why they are the way they are. In the case above, each axioms prevents something breaking later. The additive inverses ensures that cancellation works, and multiplicative inverses means that scaling can be undone, and distributivity means scaling respects addition. 

This is also the reason why $\mathbb{Z}$ is not a field, as multiplication cannot always be done. However, things of the form $\mathbb{Z}/\mathbb{Z}_p$ where $p$ is prime is a field, and they are incredibly useful for encryption\footnote{Encryption in a field... something of this reminds of a post on the cipher forum}

\section{Vector spaces}

Recalling the axioms of vector spaces from Linear Algebra I, a vector space is simply a system where: 
\begin{itemize}
    \item Objects can be added together
    \item Objects can be scaled by elements of a field
\end{itemize}

All this time, we're often used to imagining vectors as arrows, and the idea I want to convey here is that these objects don't have to be arrows. They could be functions, polynomials, matrices, sequences, etc. The only thing that matters here is the structure, not objects. 

\subsection{`Vector space over \mathbf{F}'} 

In most linear algebra textbooks, this is a phrase which is used almost all the time. So when we say `$V$ is a vector space over a field $F$', we are fixing which scalars are allowed. This will be very important as changing the field changes what `linear' means, i.e. $\mathbb{R}^n$ over $\mathbb{R}$ and $\mathbb{R}^n$ over $\mathbb{Q}$ are not the same vector space. 

On another note, changing the field doesn't change the vectors themselves, but changes what combination we're allowed to form with them. Eg: if our field was $\mathbb{Q}$, then we cannot scale vectors by an irrational number 

\section{Scalars and vectors}

In the past, we have seen scalars as just real numbers, but now we can generalise scalars to being elements of an arbitrary field $F$. 

The way I'd like to think about it, a scalar is: 
\begin{itemize}
    \item an element of the field $F$ 
    \item something that acts on the vectors
\end{itemize}

Whereas as vector is: 
\begin{itemize}
    \item an element of the vector space $V$
    \item something that gets acted upon
\end{itemize}

This is also why we have two separate distribution laws (look in notes), and why $0_F \neq 0_V$, and $1_F$ exists but there is no $1_V$ 


\section{Functions as Vectors}

Let's define a function: 
$$
\set{ f: S \rightarrow F}
$$

what this is showing us, or rather telling us, is all the functions from a set $S$ into a field $F$. Each element of this vector space is a function. 

Addition and scalar multiplication for functions then works pointwise, i.e. everything happens at each stage: 
$$
(f+g)(s) = f(s) + g(s), \quad (\lambda f)(s) = \lambda f(s) 
$$

We can go through the axioms of vector space see that this really is a vector space, but that's explained in the notes so do refer to them. 

However on that note, closure and associativity comes from the field axioms, but identity is the $0_F$. 

\subsection{0}

The zero function is: 
$$
0(s) = 0_F , \quad \forall s \in S 
$$

So it is important to notice again that $0_F$ is a zero in the field, whereas the $0_V$ is the zero function (vector). 

Funnily enough the set of all $m \times n$ matrices form a vector space. A matrix is: 
$$
A: \set{ 1,... m} \times \set{1,...n} \rightarrow F 
$$

So we can see that it's just a function from an indexed set into a  field, so $M_{m \times n} (F)$ is a vector space. 

\section{Subspaces}

Before going into what a subspace is, let's think about the word and the idea of a `sub' thing. 

A subspace, informally, is like a smaller collection of vectors forming part of a bigger vector space, which also behaves like a vector space. So you might've noticed that the subspace axioms are the same as the vector space axioms. 

\subsection{Subset vs Subspace}

Let's look at two different notations below: 
$$
W \subseteq V , \quad W \leq V
$$

The first notation, $W \subseteq V$, means that every element of $W$ is in $V$, whereas the right hand notation is for subspace that can be often seen in textbooks. 

So writing $W \leq V$ is the same as saying $W$ is a subset of $V$ and $W$ is a vector space itself under the same operations. 

The main idea here is - every subspace is a subset, but not every subset is a subspace. 

\subsection{The origin}

Similar to vector spaces, subspaces must also contain the origin. And any line not through the origin is not a subspace. So one of the axioms that a subspace satisfies is that $0_v \in W$

\subsection{Usefulness}

Now, I don't think it's immediately obvious why subspaces are useful things. But later in the course they will be very helpful in describing things like: 
\begin{itemize}
    \item Solution sets of homogeneous equations 
    \item Span of vectors 
    \item Null and column spaces 
    \item Eigenspaces 
\end{itemize}

Basically, every important structure in linear algebra is a subspace, including matrices. 

All $m \times n$ matrices form a vector space and with them, all symmetric and upper triangular are subspaces because they obey the axioms of closure under addition and scalar multiplication. 

Not only this, all sequences and function spaces are also vector spaces, examples of which can be found in the notes. 


\chapter{Bases}

\section{The Problem}

Before defining anything, we must ourselves a very important question: if vectors are just abstract objects, how do we describe them? 

In $\mathbb{R}^2$, we have $(x,y)$ coordinates which we can write as: 
$$
(x,y) = x(1,0) + y(0,1)
$$

These coordinates come from the standard basis vectors of $\mathbb{R}^2$, namely $(1,0), (0,1)$

However, when now working with spaces of polynomials, matrices and functions, there is no obvious $(1,0)$ arrow. So the problem, or the question that bases address is how we describe vectors abstractely and systematically in an abstract vector space. 

\section{Building Blocks to Bases}

Let's recall a few definitions from Linear Algebra I, I promise this will help us when we eventually come to defining what a basis is. 

\subsection{Linear Combination}
An element $v \in V$ is a \textbf{linear combination} if $\exists a \in F$ such that: 
$$
v = a_1v_1 +... + a_n v_n
$$

This is essentially how we build new vectors from old ones, adding scalars from a field. 

\subsection{Span}
A \textbf{span} defined as: 
$$
\text{span} \set{v_1... v_n} = \set {a_1 v_1+ ... + a_nv_n | a_1...a_n \in F}
$$

is simply a set of all linear combinations. So if a linear combination is building new vectors, a span is the set of everything we can build from those vectors. 

Here's an important note we'll visit later: \textbf{The span of a set is the smallest subspace containing that set}

The proof of the above can be found in notes, what it's saying is that the span of a set is itself a subspace, and that any smaller subspace than the span cannot contain all linear combinations. 

Think about this intuitively for a minute, the span contains all the possible vectors we can reach as it has all the possible linear combinations, so it acts kind of linear a container for those vectors. 

\subsection{Linear Independence}

Vectors are said to be \textbf{Linearly independent} $\iff$
$$
a_1 v_1 +... a_n v_n = 0 \implies a_1 = ... = a_n = 0
$$

What this means is that no vectors can be built as a linear combination of other vectors. 

\section{Defining a Basis}

Now we have all the definitions we need to define a basis. 

In simple terms, a \textbf{basis} is a set of vectors that \textbf{both} spans the space and is linear independent, i.e. a basis is the smallest complete set of vectors we need to build our vector space. 

I'd like you to think why we need both conditions to be true here. The problem we were trying to solve is finding a system to represent vectors so that we have enough vectors to reach everything, and few enough vectors so that representation is unique. Notice how both of those criteria are just the definition of span and linear independence. 

\section{The Coordinate Problem}

Going backwards the, if $B = \set {v_1.. v_n}$ is a basis of $V$, then every vector $v \in V$ can be written uniquely as a linear combination: 
$$
v = a_1v_1 +... + a_n v_n
$$

So once a basis is fixed, every vectors gets assigned coordinates and more importantly, those coordinates are unique. What we have in this case is now a \textbf{tuple of scalars}

Once we choose a basis, i..e choosing a coordinate system for our vector space, we can represent any vector as $(a_1...a_n)$ such that: 
$$
V \cong F^n
$$

Isomorphism is something that will be covered in more detail later, but the idea here is that we can turn any abstract object into coordinate tuples, which is incredibly powerful. 

\chapter{Linear Transformations}

\chapter{Determinants}

\chapter{Diagonalisability}


